# POET Bench 核心要点与数据流程

## 从技术跑分到价值创造——为什么需要新的Benchmark

我们正步入一个由大型语言模型（LLM）驱动的新人工智能时代。以LLM为核心的智能体（Agent）系统，正以前所未有的潜力解放并重塑知识工作。然而，一个严峻的现实是，当前主流的LLM评测体系与其实际商业价值之间存在显著的脱节。众多在开源排行榜上表现优异的模型，在真实应用中却不及Claude等商业模型受欢迎。
这揭示了两个核心问题：
1. 过度优化（Overfitting）：许多模型可能针对特定评测基准进行过度优化，导致榜单性能虚高，却忽视了在广泛实际应用场景中的泛化能力。
2. 价值盲区（Value Gaps）：现有评测基准未能全面覆盖真实世界，特别是企业场景对模型能力的复杂需求。
尽管如DeepResearch Bench和WideSearch等基准极大地推动了通用智能体能力的发展，但它们与真实的企业级应用场景仍存在显著差异：
- 知识来源的混合性：企业决策需深度整合内部私域知识（如财报、客户数据）与外部公域信息。
- 任务约束的复杂性：企业任务常附带严格的格式、合规、安全与逻辑约束。
- 价值导向的明确性：评估的最终标准是商业洞察的价值、决策建议的可行性，而非学术深度。

为应对这一挑战，POET基准的设计理念是“始于终局”。我们并非从技术能力出发，而是首先回答一个根本问题：一个值得企业信赖并为其付费的AI智能体，需要满足决策者哪些最核心的关切？
通过与行业专家的深度合作，我们总结出企业在引入AI智能体时的四大根本性、非黑即白的需求。这四大关切，构成了POET基准的评测基石：
1. 效果为王 (Effectiveness over Novelty)：一切围绕真实业务场景。准确性、深度和可用性是唯一的价值尺度。不能解决实际问题的炫技毫无意义。
2. 信任与验证 (Trust through Transparency)：决策者需要为结果负责，因此必须能对AI的产出进行复核。“提供来源”、“列出依据”是建立信任的生命线，可验证性是其商业化的基石。
3. 价值提升 (Value Creation over Efficiency)：终极目标不仅是“省时间”，更是“赚更多钱、省更多钱、避免巨大损失”。智能体必须能直接作用于企业的利润表和风险控制。
4. 可靠性与稳定性 (Reliability as a Utility)：智能体必须像电力系统一样稳定、可靠。任何一次在关键任务上的失误都可能是不可接受的。
正是基于对这些核心关切的深刻理解，我们设计了POET 深度研究基准。它的每一个环节，都旨在回答上述问题，从而衡量AI智能体的真实商业投资回报率（ROI）。

## 定义商业价值——如何筛选高价值评测Query

一个评测基准的起点，是定义什么样的问题（Query）才值得被评测。POET首先建立了一套标准化、可量化的框架，用于筛选最能体现AI智能体商业价值的评测Query。
 1.1 任务设计三大原则

我们的任务设计摒弃了静态、学术化的测试集，而是与行业专家合作，从真实的业务中提取任务，遵循三大核心原则：
- 原则一：评估由需求定义 (Evaluation is defined by demand)
- 我们深入了解特定职业的完整业务流程，通过2x2矩阵（维度：AI可行性、结果可评估性）筛选出AI能做且结果有明确标准的核心任务，作为基准的焦点。
- 原则二：“实时”收集评估任务 (Evaluation tasks are collected “live”)
- 评估任务直接从合作企业的真实业务中积累，并分为静态任务（核心技能）和动态任务（依赖当前市场环境），确保评估环境与市场同步，避免时效性问题。
- 原则三：领域价值驱动评估目标 (Domain value drives evaluation objectives)
- 每个任务都锚定真实的商业价值。我们通过记录“人类专家完成该任务所需时间”并结合“专家的薪资水平”，来估算每个任务的商业价值，为衡量AI的效用（如成本节约）提供了直观指标。
当前query数据的领域分布情况如下图：
[图片]
1.2 query 核心评估维度

我们设计了七个加权维度来精确捕捉一个Query的商业价值。只有在高价值Query上的出色表现，才能证明智能体的真正实力。
维度        权重        描述        评分标准（1分 -> 5分）
A. 决策颠覆性        5%        该问题的答案对最终商业决策的影响程度。        1. 参考级 -> 5. 决定性：答案从“仅供参考”的信息，到成为决策的核心依据和唯一理由。
B. 分析复杂性        20%        解决问题所需的认知复杂度与步骤。        1. 简单检索 -> 5. 多维建模：从单点查询，到需要交叉验证、建立量化模型、进行逻辑推演。
C. 行动导向性        20%        答案的可执行程度，能否直接转化为行动。        1. 描述性 -> 5. 可执行：答案从一段文字描述，到一个包含步骤、清单、模型或完整方案的可行动输出。
D. 风险/收益规模        15%        该问题背后所关联的潜在商业收益或风险损失的大小。        1. 微不足道 -> 5. 战略级：关联的财务影响从可忽略不计，到影响公司数亿资金流向或存亡。
E. 时效敏感性        10%        答案对信息新鲜度的依赖程度。        1. 静态知识 -> 5. 实时情报：从基于历史事实，到极度依赖24小时内的动态信息（如股价、舆情、政策）。
F. 专业壁垒        5%        解决问题所需知识的专业性和稀缺性。        1. 通用知识 -> 5. 尖端领域：从常识，到需要深度的、鲜为人知的领域专业知识（如特定专利法条、临床诊疗指南）。
G. 可验证性        25%        答案正确与否是否易于被人类专家快速验证。        1. 难以验证 -> 5. 极易验证：从输出一个模糊观点，到输出带有数据源、计算过程、法律条文引用的可验证答案。
暂时无法在飞书文档外展示此内容
总分计算公式：
Query价值分=∑(维度得分×维度权重)
通过该模型，我们确保POET基准中的每一个任务都经过了商业价值的严格筛选，从而使评测结果与企业的真实痛点和需求直接挂钩。
如下是query价值评分结果，我们可以取>=4.5分的前20条结果作为初始query set
暂时无法在飞书文档外展示此内容

暂时无法在飞书文档外展示此内容
原始数据
query_scores.json


---
## POET评估框架——如何衡量智能体价值

筛选出有价值的问题后，我们如何评测智能体的答案？POET提出了一套创新的“与专业对齐”（Profession-Aligned）的评估方法，其核心是评估智能体的技术-市场匹配度（Tech-Market-Fit）。我们不再问“这个AI有多强？”，而是问“这个AI能为我创造多少价值？”。
为此，我们构建了包含三大维度的价值评估框架：
3.1 效率价值 (Efficiency Value)：解放专家生产力的经济学算式
量化智能体最直接的经济贡献，回答“快不快，省不省”的问题。
- 任务完成时间 (Time-to-Completion)：对比智能体与领域专家的耗时，量化“解放”出的专家时间。
- 任务完成Token用量：完成该任务使用的token数量。
- 人力替代率 (Automation Rate)：衡量无需人工干预的自动化节点比例。
- 运营成本节约 (Cost Saving)：精确计算智能体带来的净成本节约。

3.2 质量价值 (Quality Value)：构建人机协作的信任基石
RACE：成果的宏观品质评估 
RACE (Reference-based Adaptive Criteria-driven Evaluation with Dynamic Weighting) 框架提供了一种对生成报告的整体性、结构化和专业性的宏观审视。它超越了简单的“正确/错误”二元判断，深入评估最终交付物的综合品质。它摒弃了传统固定、死板的评分标准，采用了一种更智能、更自适应的“LLM-as-a-Judge”方法。
RACE评测包含以下四个维度的标准：

1.  全面性 (Comprehensiveness): 信息覆盖的广度、深度和相关性。
2.  洞察力 (Insight): 分析的深度、独到性、逻辑性和结论价值。
3.  指令遵循能力 (Instruction Following): 报告是否准确、完整地回应了任务的所有要求和限定条件。
4.  可读性 (Readability): 结构清晰度、语言流畅度、数据呈现效果和整体易理解性。

- 动态权重与自适应标准针对每一个研究任务，RACE首先会让“裁判LLM”动态生成一套专属的评估维度权重和细化的评分标准。例如，对于一个要求投资可行性分析的任务，“洞察深度”的权重就可能高于“文章可读”。以下是经前文所述筛选得到的前20条query的rubrics：
暂时无法在飞书文档外展示此内容
原始数据
query_rubrics.json
- 基于参考的评分为了避免“裁判LLM”打分过于宽松，RACE引入了“参考答案”机制。它会将待评估的报告与一篇高质量的参考报告进行对比，从而给出一个更具区分度的相对分数。
这种方法不仅能全面评估报告的全面性、深度、指令遵循度和可读性，还能确保评估结果的公正与可靠。
FACT：事实的微观溯源校验 
FACT (Factual Abundance and Citation Trustworthiness) 框架则专注于评估AI代理在信息检索和引用方面的能力。毕竟，一份优秀的报告必须建立在真实、可信的事实基础之上。如果事实基础不可信，报告将被一票否决。
- 事实与来源的精准提取FACT首先会自动从报告中提取出每一个陈述（statement）及其对应的引用来源URL。
- 自动化事实核查随后，“裁判LLM”会访问这些URL，检查网页内容是否真的支持报告中的陈述，并给出“支持”或“不支持”的判断。
- 两大核心指标最后，框架会计算出两个关键指标：
  - 有效引用数 (Effective Citations)：平均每篇报告包含多少条有事实依据支撑的引用。这个数字越高，说明报告内容越充实。
  - 引用准确率 (Citation Accuracy)：所有引用中，有多少比例是准确无误的。这个比例越高，说明代理越可靠，没有“胡编乱造”。

3.3 战略价值 (Strategic Value)：驱动组织进化的知识飞轮

评估智能体超越单次任务的、对组织能力的长期赋能。
- 复杂任务处理能力：能否独立处理人类专家需半小时以上才能完成的深度研究。
- 知识沉淀与复用：能否将任务产出沉淀到知识库，并在后续任务中智能复用，形成“知识飞轮”。

综上，POET的价值框架共同构建了一个全面、深刻且与商业目标强相关的评估体系，推动AI评估从孤立的功能测试走向综合的价值衡量。
